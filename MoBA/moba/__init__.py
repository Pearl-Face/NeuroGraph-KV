from functools import partial
from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS
import transformers
from .wrapper import moba_layer, Qwen2MoBAAdaptor 
from .moba_naive import moba_attn_varlen_naive
from .moba_efficient import moba_attn_varlen
from .config import MoBAConfig


def register_moba(cfg: MoBAConfig):
    ALL_ATTENTION_FUNCTIONS["moba_naive"] = partial(moba_layer, moba_attn_varlen_naive, cfg)
    ALL_ATTENTION_FUNCTIONS["moba"] = partial(moba_layer, moba_attn_varlen, cfg)

    print(f"âš¡ [MoBA] Injecting Qwen2FlashAttention2 with Chunk={cfg.moba_chunk_size}, TopK={cfg.moba_topk}")
    
    import transformers.models.qwen2.modeling_qwen2 as qwen2_module
    
    print(f"ğŸš€ [MoBA] Injecting MoBA into Qwen2...")

    # 1. æ³¨å…¥é…ç½®å’Œå®ç°å‡½æ•°
    # ç¡®ä¿ Qwen2MoBAAdaptor èƒ½å¤Ÿæ¥æ”¶æ¥è‡ª FlashAttention2 è·¯å¾„çš„å‚æ•°
    Qwen2MoBAAdaptor.moba_config = cfg
    Qwen2MoBAAdaptor.moba_impl = staticmethod(moba_attn_varlen)
    
    # 2. å…¨é‡åŠ«æŒï¼šæ— è®º transformers é€‰å“ªä¸ªç±»ï¼Œéƒ½å¼ºåˆ¶è·³è½¬åˆ°æˆ‘ä»¬çš„ Adaptor
    # è¿™ä¸€æ­¥éå¸¸å…³é”®ï¼Œå®ƒä¿è¯äº†å³ä½¿åœ¨ flash_attention_2 è·¯å¾„ä¸‹ä¹Ÿè¿è¡Œ MoBA
    qwen2_module.Qwen2FlashAttention2 = Qwen2MoBAAdaptor
    qwen2_module.Qwen2Attention = Qwen2MoBAAdaptor
    qwen2_module.Qwen2SdpaAttention = Qwen2MoBAAdaptor

    # 3. å…¼å®¹æ€§è¡¥ä¸ï¼šæœ‰äº›ç‰ˆæœ¬çš„ transformers ä¼šç»´æŠ¤ä¸€ä¸ªå†…éƒ¨ç±»æ˜ å°„è¡¨
    if hasattr(qwen2_module, "QWEN2_ATTENTION_CLASSES"):
        qwen2_module.QWEN2_ATTENTION_CLASSES = {
            "eager": Qwen2MoBAAdaptor,
            "flash_attention_2": Qwen2MoBAAdaptor,
            "sdpa": Qwen2MoBAAdaptor,
        }
    
    print(f"âœ… MoBA is now shielding both Eager and Flash-Attn paths.")
